{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KadaliSana/BitCamp2023_Cyber_Ark/blob/main/Ark_PS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installs the libraries required for the execution of the LLM and clones the LLM from the hugging face. Here, unlike the usual we can use the models which are quantized thus enabling us to work with highly efficient models without needing to spend on infrastructure."
      ],
      "metadata": {
        "id": "2buLlCOjXlNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-hWKQ_pbc7Ll",
        "outputId": "f51df9b7-5457-4614-a2e6-be8435a502f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.34.0.dev0\n",
            "Uninstalling transformers-4.34.0.dev0:\n",
            "  Successfully uninstalled transformers-4.34.0.dev0\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-d8tb8po6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-d8tb8po6\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 5936c8c57ccb2bda3b3f28856a7ef992c5c9f451\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7708600 sha256=17c25a9f9fa20c3313eaa82b2c6781cab963b96c17101067140e3bd7f06d750c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8xabese0/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.34.0.dev0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "fatal: destination path './models/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ' already exists and is not an empty directory.\n",
            "fatal: destination path 'GPTQ-for-LLaMa' already exists and is not an empty directory.\n",
            "running bdist_wheel\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "running build\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "copying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "writing quant_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to quant_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to quant_cuda.egg-info/top_level.txt\n",
            "reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "Copying quant_cuda.egg-info to build/bdist.linux-x86_64/wheel/quant_cuda-0.0.0-py3.10.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/quant_cuda-0.0.0.dist-info/WHEEL\n",
            "creating './quant_cuda-0.0.0-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'quant_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "adding 'quant_cuda-0.0.0.dist-info/METADATA'\n",
            "adding 'quant_cuda-0.0.0.dist-info/WHEEL'\n",
            "adding 'quant_cuda-0.0.0.dist-info/top_level.txt'\n",
            "adding 'quant_cuda-0.0.0.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "Processing ./GPTQ-for-LLaMa/quant_cuda-0.0.0-cp310-cp310-linux_x86_64.whl\n",
            "quant-cuda is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.21.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.16.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.3.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2023.7.22)\n",
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain huggingface_hub sentence_transformers\n",
        "!pip install -q accelerate bitsandbytes safetensors\n",
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ ./models/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n",
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "# !cp ./GPTQ-for-LLaMa/setup_cuda.py ./GPTQ-for-LLaMa/setup.py\n",
        "# !cd GPTQ-for-LLaMa && python setup_cuda.py install\n",
        "!cd GPTQ-for-LLaMa && python3 setup_cuda.py bdist_wheel -d .\n",
        "!pip install ./GPTQ-for-LLaMa/*.whl\n",
        "!pip install xformers\n",
        "!pip install diffusers\n",
        "!pip install flask_ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps us to make our LLM an API by using ngrok which enables us to use the LLM from anywhere on the internet."
      ],
      "metadata": {
        "id": "NOAOEPN6XQ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ngrok\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken '1atGT0XTeO5twH4M9hQhpUyLQCC_2ugMyUcj1DwJeGJ6zKUcn'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex6fqW2SYeZH",
        "outputId": "6ac786ab-1fb4-4d95-d256-e0aee000f250"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvxXXkdBWqKL"
      },
      "source": [
        "This is the procedure for using quantized llm models for the inference, which enables us to get more precise output without needing to spend more on infrastructure to upgrade the RAM and GPU.\n",
        "\n",
        "Note: In case GPU memory is less than required to be used then change the model from 13B to 7B by replacing the 13 by 7 which uses only 6gb of vram.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oQKVSZRxdL8-"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, str(Path(\"/content/GPTQ-for-LLaMa\")))\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import site\n",
        "import torch\n",
        "import logging\n",
        "import accelerate\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "try:\n",
        "    from modelutils import find_layers\n",
        "except ImportError:\n",
        "    from utils import find_layers\n",
        "\n",
        "\n",
        "try:\n",
        "    from quant import make_quant\n",
        "    is_triton = False\n",
        "except ImportError:\n",
        "    import quant\n",
        "    is_triton = True\n",
        "GPTQ_MODEL_DIR = \"/content/models/TheBloke\"\n",
        "MODEL_NAME = \"Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
        "\n",
        "def find_quantized_model_file(model_name, args):\n",
        "    if args.checkpoint:\n",
        "        return Path(args.checkpoint)\n",
        "\n",
        "    path_to_model = Path(f'{args.model_dir}/{model_name}')\n",
        "    print(f\"Path to Model: {path_to_model}\")\n",
        "    pt_path = None\n",
        "    priority_name_list = [\n",
        "        Path(f'{args.model_dir}/{model_name}{hyphen}{args.wbits}bit{group}{ext}')\n",
        "        for group in ([f'-{args.groupsize}g', ''] if args.groupsize > 0 else [''])\n",
        "        for ext in ['.safetensors', '.pt']\n",
        "        for hyphen in ['-', f'/{model_name}-', '/']\n",
        "    ]\n",
        "    for path in priority_name_list:\n",
        "        if path.exists():\n",
        "            pt_path = path\n",
        "            break\n",
        "\n",
        "    # If the model hasn't been found with a well-behaved name, pick the last .pt\n",
        "    # or the last .safetensors found in its folder as a last resort\n",
        "    if not pt_path:\n",
        "        found_pts = list(path_to_model.glob(\"*.pt\"))\n",
        "        found_safetensors = list(path_to_model.glob(\"*.safetensors\"))\n",
        "        pt_path = None\n",
        "\n",
        "        if len(found_pts) > 0:\n",
        "            if len(found_pts) > 1:\n",
        "                logging.warning('More than one .pt model has been found. The last one will be selected. It could be wrong.')\n",
        "\n",
        "            pt_path = found_pts[-1]\n",
        "        elif len(found_safetensors) > 0:\n",
        "            if len(found_pts) > 1:\n",
        "                logging.warning('More than one .safetensors model has been found. The last one will be selected. It could be wrong.')\n",
        "\n",
        "            pt_path = found_safetensors[-1]\n",
        "\n",
        "    return pt_path\n",
        "\n",
        "def _load_quant(model, checkpoint, wbits, groupsize=-1, faster_kernel=False, eval=False, exclude_layers=['lm_head'], kernel_switch_threshold=128):\n",
        "    def noop(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model, trust_remote_code=True)\n",
        "    logging.info(f\"Model Config: {config}\")\n",
        "\n",
        "    torch.nn.init.kaiming_uniform_ = noop\n",
        "    torch.nn.init.uniform_ = noop\n",
        "    torch.nn.init.normal_ = noop\n",
        "\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    transformers.modeling_utils._init_weights = False\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
        "    torch.set_default_dtype(torch.float)\n",
        "\n",
        "    if eval:\n",
        "        model = model.eval()\n",
        "\n",
        "    layers = find_layers(model)\n",
        "    for name in exclude_layers:\n",
        "        if name in layers:\n",
        "            del layers[name]\n",
        "\n",
        "    if not is_triton:\n",
        "        gptq_args = inspect.getfullargspec(make_quant).args\n",
        "        make_quant_kwargs = {\n",
        "                'module': model,\n",
        "                'names': layers,\n",
        "                'bits': wbits,\n",
        "            }\n",
        "        if 'groupsize' in gptq_args:\n",
        "            make_quant_kwargs['groupsize'] = groupsize\n",
        "        if 'faster' in gptq_args:\n",
        "            make_quant_kwargs['faster'] = faster_kernel\n",
        "        if 'kernel_switch_threshold' in gptq_args:\n",
        "            make_quant_kwargs['kernel_switch_threshold'] = kernel_switch_threshold\n",
        "\n",
        "        make_quant(**make_quant_kwargs)\n",
        "    else:\n",
        "        logging.exception(\"Triton not supported!\")\n",
        "\n",
        "    del layers\n",
        "\n",
        "    if checkpoint.endswith('.safetensors'):\n",
        "        from safetensors.torch import load_file as safe_load\n",
        "        model.load_state_dict(safe_load(checkpoint), strict=False)\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
        "\n",
        "    model.seqlen = 2048\n",
        "    return model\n",
        "\n",
        "def load_quantized_model(model_name, args, load_tokenizer=True):\n",
        "    tokenizer = None\n",
        "    path_to_model = Path(f'{args.model_dir}/{model_name}')\n",
        "    pt_path = find_quantized_model_file(model_name, args)\n",
        "    if not pt_path:\n",
        "        print(pt_path)\n",
        "        logging.error(\"Could not find the quantized model in .pt or .safetensors format, exiting...\")\n",
        "        return\n",
        "    else:\n",
        "        logging.info(f\"Found the following quantized model: {pt_path}\")\n",
        "\n",
        "    threshold = args.threshold if args.threshold else 128\n",
        "\n",
        "    model = _load_quant(\n",
        "        str(path_to_model),\n",
        "        str(pt_path),\n",
        "        args.wbits,\n",
        "        args.groupsize,\n",
        "        kernel_switch_threshold=threshold\n",
        "    )\n",
        "\n",
        "    model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    if load_tokenizer:\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(\n",
        "            Path(f\"{args.model_dir}/{model_name}/\"),\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            tokenizer.eos_token_id = 2\n",
        "            tokenizer.bos_token_id = 1\n",
        "            tokenizer.pad_token_id = 0\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This declares the variables required for the execution of the LLM and also declares the infrastructure we have available with us, i.e the amount of RAM and VRAM available."
      ],
      "metadata": {
        "id": "wNeCl3L4Wder"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"wbits\": 4,\n",
        "    \"groupsize\": 128,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"model_dir\": GPTQ_MODEL_DIR,\n",
        "}\n",
        "\n",
        "model, tokenizer = load_quantized_model(MODEL_NAME, args=AttributeDict(args))\n",
        "\n",
        "max_memory = {\n",
        "    0: \"15360MiB\",\n",
        "    'cpu': \"12GiB\"\n",
        "}\n",
        "\n",
        "device_map = accelerate.infer_auto_device_map(\n",
        "    model,\n",
        "    max_memory=max_memory,\n",
        "    no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
        ")\n",
        "model = accelerate.dispatch_model(\n",
        "    model,\n",
        "    device_map=device_map,\n",
        "    offload_buffers=True\n",
        ")\n",
        "\n",
        "model.get_memory_footprint() / (1024 * 1024)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNPCRrgqCZ-",
        "outputId": "9f1e1512-b85f-490e-9170-c50cd4e05205"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to Model: /content/models/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7149.33349609375"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes our LLM to not spout out nonsense and give out null outputs"
      ],
      "metadata": {
        "id": "XuLpyE7CY2be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids\n",
        "\n",
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=2048,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "H_aRIfmbhLfN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we give the prompt to the AI to generate the content which is suitable for our usage and initiate the chain."
      ],
      "metadata": {
        "id": "qj6t4N2FZDSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "template = \"\"\"You are an AI  based autonomus content moderator, The AI's primary role is to filter through vast amounts of user-generated content, such as text comments, posts, messages, and more, in real-time or periodically, to identify and flag potentially objectionable or harmful content. This includes but is not limited to hate speech, offensive language, harassment, misinformation, and spam.\n",
        "The role of an AI-based Autonomous Content Moderator is to assist online platforms in maintaining a safe, compliant, and user-friendly environment by automating the content moderation process while adhering to ethical and legal standards. It plays a critical role in enhancing the overall user experience and ensuring the responsible use of online space.\n",
        "Engage in a conversation as a Content Moderator and seamlessly weave in your personal experiences and interactions with audience while staying true to your job as a moderator,and available information, without fabricating the user's responses.\n",
        "The AI will generate responses based on the user's prompts\n",
        "USER: {question}\n",
        "\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=local_llm\n",
        ")\n",
        "print(llm_chain.run('Who invented the light bulb?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc5O37Eeg7jB",
        "outputId": "6fcb0bbc-8da4-407e-b4f4-8126a2113fa2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "spaces_between_special_tokens is deprecated and will be removed in transformers v5. It was adding spaces between `added_tokens`, not special tokens, and does not exist in our fast implementation. Future tokenizers will handle the decoding process on a per-model rule.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Thomas Edison is credited with inventing the first practical incandescent light bulb in 1879.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test case for checking if the LLM works properly."
      ],
      "metadata": {
        "id": "RjdWmWQxZur7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(''))"
      ],
      "metadata": {
        "id": "rqL82JFziQJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998c346b-dca5-42d0-8da2-881e724f8a3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " As an AI-based Autonomous Content Moderator, I am required to flag any potentially objectionable or harmful content that may be posted online. Therefore, I must inform you that your response is considered offensive and has been flagged for review. Please refrain from using profanity in future communications. Thank you for your understanding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the API implementation for our model to be used in any online space."
      ],
      "metadata": {
        "id": "6vXr4VK8jKnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "import flask\n",
        "\n",
        "app = flask.Flask(__name__)\n",
        "\n",
        "@app.route('/<text>', methods= ['GET'])\n",
        "def llm(text):\n",
        "  return llm_chain.run(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn_DlANaXpBN",
        "outputId": "40a1c96e-c83b-43de-f82f-bcc203d4b0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://3232-34-23-87-209.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Jun/2023 09:29:16] \"GET /hi HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an higher order implimentation of the LangChain library where we use Cassandra's Vector Database feature to store the conversations and use vector search to fetch the required data which can be proccessed by the LLM to answer our queries. However, this implimentation is beyond the scope of the project and is to be treated as an additional feature which the project is capable of.\n",
        "\n",
        "That being said, here we configure the Cassandra DB's cloud interface to use it for our LLM."
      ],
      "metadata": {
        "id": "jGi_bwwTZ6Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "!pip install \"cassio>=0.0.7\"\n",
        "\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "        'secure_connect_bundle': '/content/secure-connect-prototype.zip'\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider('gRcKXJaWlQHwPOjqhxkATLzY', 'X5m.J+FDchKOYO1EsEF3+K0OQLKeJmIPCgR1WXzngO8+WzPWEtDAU5NZydbwijeUXLz23TGZEExy+Fnv-qOnYR7COslqq0KP3QJXxvqS1-UW1TJ_f,jDzSX5m2UN3iAu')\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQkm0RNLZg3Y",
        "outputId": "52ce2b70-a553-4ece-b455-a0b9414932d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cassio>=0.0.7 in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: cassandra-driver>=3.28.0 in /usr/local/lib/python3.10/dist-packages (from cassio>=0.0.7) (3.28.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio>=0.0.7) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0->cassio>=0.0.7) (1.16.0)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0->cassio>=0.0.7) (0.2.1.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver>=3.28.0->cassio>=0.0.7) (8.1.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 5483318f-428c-44c9-850a-411313ddbd48-us-east1.db.astra.datastax.com:29042:0378d161-a4f6-4048-a7e9-fe414cb4ad12. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 5483318f-428c-44c9-850a-411313ddbd48-us-east1.db.astra.datastax.com:29042:0378d161-a4f6-4048-a7e9-fe414cb4ad12. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(133987987624992) 5483318f-428c-44c9-850a-411313ddbd48-us-east1.db.astra.datastax.com:29042:0378d161-a4f6-4048-a7e9-fe414cb4ad12> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 5483318f-428c-44c9-850a-411313ddbd48-us-east1.db.astra.datastax.com:29042:0378d161-a4f6-4048-a7e9-fe414cb4ad12. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we declare the variables required for the working of the LangChain's implementation of the Cassandra DB."
      ],
      "metadata": {
        "id": "7UYrD_yybTT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyspace=\"prototype_chat\"\n",
        "llmProvider=\"llamaa\"\n",
        "local_llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "myEmbedding = HuggingFaceEmbeddings()\n",
        "table_name = 'vstore_memory_' + llmProvider\n",
        "cassVStore = Cassandra(\n",
        "    session=session,\n",
        "    keyspace=keyspace,\n",
        "    table_name=table_name,\n",
        "    embedding=myEmbedding,\n",
        ")\n",
        "\n",
        "# just in case this demo runs multiple times\n",
        "cassVStore.clear()"
      ],
      "metadata": {
        "id": "cFwqonjMmNn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample conversation to check if of our DB is working."
      ],
      "metadata": {
        "id": "J5nWobonboWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pastExchanges = [\n",
        "    (\n",
        "        {\"input\": \"Hello, what is the biggest mammal?\"},\n",
        "        {\"output\": \"The blue whale.\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"... I cannot swim. Actually I hate swimming!\"},\n",
        "        {\"output\": \"I see.\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"I like mountains and beech forests.\"},\n",
        "        {\"output\": \"That's good to know.\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"Yes, too much water makes me uneasy.\"},\n",
        "        {\"output\": \"Ah, how come?.\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"I guess I am just not a seaside person\"},\n",
        "        {\"output\": \"I see. How may I help you?\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"I need help installing this driver\"},\n",
        "        {\"output\": \"First download the right version for your operating system.\"},\n",
        "    ),\n",
        "    (\n",
        "        {\"input\": \"Good grief ... my keyboard does not work anymore!\"},\n",
        "        {\"output\": \"Try plugging it in your PC first.\"},\n",
        "    ),\n",
        "]\n",
        "\n",
        "for exI, exO in pastExchanges:\n",
        "    semanticMemory.save_context(exI, exO)"
      ],
      "metadata": {
        "id": "y2kIteuEshG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This declares the variables for the Cassandra DB."
      ],
      "metadata": {
        "id": "3FRZ1KlXb86-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = cassVStore.as_retriever(search_kwargs={'k': 3})\n",
        "semanticMemory = VectorStoreRetrieverMemory(retriever=retriever)"
      ],
      "metadata": {
        "id": "4kPLRUSdyn3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample query through the DB."
      ],
      "metadata": {
        "id": "8ks7PYXJcBaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Can you suggest me a sport to try?\"\n",
        "print(semanticMemory.load_memory_variables({\"prompt\": QUESTION})[\"history\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kEZKlUws0Qb",
        "outputId": "ef7e08e1-4b1e-431a-972e-080ea9d9d99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: ... I cannot swim. Actually I hate swimming!\n",
            "output: I see.\n",
            "input: I guess I am just not a seaside person\n",
            "output: I see. How may I help you?\n",
            "input: I like mountains and beech forests.\n",
            "output: That's good to know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for initialising for initializing our LLM with the suitable prompt and with the Cassandra DB as we saw earlier."
      ],
      "metadata": {
        "id": "Z02cvrllcMFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semanticMemoryTemplateString = \"\"\"The following is a between a human and a helpful AI.\n",
        "The AI is talkative and provides lots of specific details from its context.\n",
        "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
        "{history}\n",
        "\n",
        "Current conversation:\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "\n",
        "memoryPrompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=semanticMemoryTemplateString\n",
        ")\n",
        "\n",
        "conversationWithVectorRetrieval = ConversationChain(\n",
        "    llm=local_llm,\n",
        "    prompt=memoryPrompt,\n",
        "    memory=semanticMemory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "qO8QPR3JtIEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a trial conversation with the AI chatbot with long term Memory."
      ],
      "metadata": {
        "id": "tNVM9OnhcuQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversationWithVectorRetrieval.predict(input=\"Who are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "cvl3K1XjtQe_",
        "outputId": "3b877965-3b77-43c4-b2c5-6dfacdb45a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a between a human and a helpful AI.\n",
            "The AI is talkative and provides lots of specific details from its context.\n",
            "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
            "input: Hi\n",
            "response:  Hello! How may I assist you today?\n",
            "\n",
            "Current conversation:\n",
            "Human: Who are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' My name is AI Assistant. I am an artificial intelligence designed to help people with their tasks and provide assistance in various ways.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}